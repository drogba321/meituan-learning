###MySQL查询优化的方式###

- 建立索引
	+ MyISAM存储引擎的索引包括.frm文件、.myd文件、.myi文件等，它的索引文件和普通数据文件是分开存储的。
	+ InnoDB存储引擎的索引包括.idb文件，它的索引文件和普通数据文件是放在一起存储的。
	+ 某些情况下，建索引并不需要为整个字段建立索引，而仅仅为该字段的前若干个字符（例如前20个字符）建立索引就能满足要求。
	
- 设置缓存
	+ 最常见的是缓存SQL语句及对应的查询结果，等到下次执行相同的SQL语句时，直接从缓存中取出查询结果即可。
	+ 上述缓存方式在数据经常发生变动的情形下不应当使用，因为缓存的查询结果和实际的查询结果经常会不一致。
	
- MySQL慢查询
	+ 通过使用MySQL慢查询日志，并指定一个时间阈值，可以将执行时间大于该阈值的SQL语句写入日志，供人们分析并跟踪这些SQL语句执行效率低下的问题。
	+ MySQL慢查询日志有时候会对人们的分析产生误导，例如一个原本很简单的SQL语句却执行了很久，实际原因是在执行该SQL语句的同时，还在执行其他的写操作，由于写操作对数据进行加锁，因此该SQL语句迟迟无法访问数据，导致执行时间过长。这种问题往往很难在慢查询日志中直接观察出来。
	
- 分库分表
	+ 数据库分表有两种方式：横向分表和纵向分表。
	+ **横向分表**是将原表的若干行作为一个单独的表，例如原表有1千万条数据，横向分为10个表，每个表存储100万条数据。
	+ **纵向分表**是将原表的某些字段单独建立一个表，例如原表中有一个text格式的大字段，这个大字段会严重拖慢整张表的查询速度，因此可以把这个大字段单独拎出来放到一张表里，通过外键与原表关联，这样就可以加快原表的查询速度。
	
- 主从分离
	+ 写操作是写入主表的，读操作是从从表中读取，通过特定的同步机制来保持主、从表的数据一致性。
	
- 补充
	+ MyISAM存储引擎不支持事务，因此它为了保证数据操作的安全性和一致性，会在写操作的时候对整张表加锁，故它不支持行级锁。
	+ InnoDB存储引擎支持事务，因此它本身就可以保证数据操作的安全性和一致性，无需在写操作的时候对整张表进行加锁，仅对写入的当前行加锁即可，故它支持行级锁。
	
###memcached缓存###

- 缓存算法
	+ 缓存算法用来决定当缓存空间已满时，应当淘汰哪些数据。常用的缓存算法策略包括FIFO、LRU、LFU等等。
	+ FIFO（先进先出）：**最先进入缓存的数据被淘汰**，这是最简单的缓存策略。该算法采用普通队列数据结构实现。
	+ LFU（Least Frequently Used，最近最少使用算法）：**最近一段时间内，使用次数最少（也即使用频率最低）的数据被淘汰**。这种算法实现的时候需要为每一个数据维护一个访问次数值，每次访问一个数据时，该值增1，当缓存空间已满时，删除访问次数值最小的数据。
	+ LRU（Least Recently Used，最近最久未使用算法）：**最久没有被使用的数据被淘汰**。这种算法可以使用一个链表来实现：链表的头部代表最新的数据，尾部代表最旧的数据（即最久没有被使用的数据），新插入的数据放在链表头，每次访问缓存中的一个数据时将该数据移至链表头，当链表已满时删除链表尾部的数据。
	+ LRU-k算法：LRU的改进算法，它把“最久没有被使用过一次”的情形扩展到了“最久没有被使用过k次”的情形，只有当一个数据被访问超过k次后，才放入缓存；当缓存已满时，淘汰掉“第k次访问时间距当前时间最大”（也即“最久没有被使用过k次”）的数据。它的实现需要维护两个队列：第一个队列用来维护历史访问数据，并对每个数据统计访问次数，当次数达到k后才将该数据放入第二个队列（LRU队列，它才是真正的缓存），若历史访问数据的访问次数一直达不到k，则会在第一个队列满的时候被淘汰掉；对于进入第二个队列的真正缓存数据，当再次被访问时，则按照上述LRU策略对数据重新排序（每次把被访问的数据移至队列头），当缓存已满时，删除队列尾部的数据。一般使用LRU-2算法性能最优。
	+ 2Q算法（Two Queue）：采用两个队列，一个采用FIFO策略，一个采用LRU策略。当数据第一次被访问时，放入FIFO队列；若该数据再次被访问，则移至LRU队列（真正的缓存队列）。
	+ MQ算法（Multi Queue）：采用多个队列，每个队列都有不同的优先级，每个队列均采用上述LRU策略管理数据。新插入的数据放入优先级最低的第一个队列，当被访问达到一定次数后，将该数据放入优先级更高的第二个队列；当访问次数继续增长一定程度后，将该数据移至优先级更高的第三个队列，以此类推。访问次数越多，则所处的队列优先级越高。若一个数据长时间没有被访问，则需要降低优先级（放回优先级较低的队列），直至最终从优先级最低的队列中被删除。

	
- 一致性哈希
	+ 一致性哈希算法用于分布式集群的数据散列存储，分布式集群中的节点（机器）与待存储的数据一样，都要散列到一个**圆环**上，圆环大小通常为2^32。对节点进行哈希的输入值一般为节点IP或节点机器的唯一别名。根据散列的位置，把数据存储在**顺时针方向离自己最近的节点**上。
	+ 当新增一个节点时，该节点也要散列到圆环上的某一位置，只有该节点与其逆时针方向的相邻节点之间的数据才需要改变存储节点，其余的数据都不需要有任何变化（因为它们顺时针方向最近的节点并没有改变）。
	+ 当删除一个节点时，与新增节点的情形类似，也是只有该节点与其逆时针方向相邻节点之间的数据需要改变存储节点，其余数据不受影响。
	+ 从上述分析可以看出，一致性哈希算法可以用最小的代价完成新增/删除节点之后的重新哈希。
	+ 为了保证数据存储的平衡性（每个节点都要存储数量差不多的数据），一致性哈希算法中引入了**虚拟节点**，一个物理节点可以映射到多个虚拟节点上。对虚拟节点进行哈希的输入值一般为节点IP或节点机器的唯一别名＋数字1、2、3…。原本物理节点在圆环上的分布可能很不均衡，但是经过到多个虚拟节点的映射后，这些虚拟节点在圆环上的分布会趋向于均衡。因此每个虚拟节点存储的数据量不会相差太多，从而提高平衡性。
	
###其他杂记###

- Linux命令补充
	+ scp：向远程机器传送文件。cp命令只能本地拷贝文件，而scp命令可以拷贝文件到远程机器。
	+ mv：不仅用于移动文件，还可以用于重命名文件，只要源路径和目的路径一样即可。
	+ \> pbcopy：用于Mac系统，复制内容到剪贴板，例如`cat a.txt > pbcopy`。
	+ pbpaste \>：用于Mac系统，将剪贴板的内容写入某个地方，例如`pbpaste > b.txt`。
	+ wc：用于统计文本文件中的字数、行数等信息，`wc -l <filename>`统计行数，`wc -c <filename>`统计字节数，`wc -m <filename>`统计字符数，等等。
	+ sort：用于将文本文件中的内容按行排序，最终升序输出。加入-u选项可以在输出结果中去除重复行，加入-r选项可以按降序输出。
	+ uniq：去除文本文件中的重复行，一般与sort命令一起使用，例如`cat a.txt | sort | uniq > b.txt`。
	+ awk：文本分析工具，把文本文件逐行读入，以空格为默认分隔符将每行切片，切开的部分再进行各种分析处理，例如`cat a.txt | awk  '{print $1}'`，只输出每行中第一个切片的内容。

- git tag
	
	+ 查看历史tag记录：`git tag`。
	+ 打tag：`git tag -a <版本名称> -m <注释信息>`.
	+ 推送本地打好的tag到远程：`git push --tags`。（因为普通的push操作不会把本地打好的tag推送到远程，因此需要加--tags选项）。
	
- Cookie与Session
	+ 在基于HTTP无状态协议的B/S架构中，为了保存用户信息，可以采用Cookie或Session。可以只有Cookie没有Session；可以只有Session没有Cookie；也可以既有Cookie又有Session。如何选择应当根据业务场景来定。